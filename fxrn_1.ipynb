{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth, get_bin_seeds\n",
    "from scipy.linalg    import hankel\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "from util.ZigZag import ZigZag, ZigZag0\n",
    "from util.CalcReturns import CalcReturns\n",
    "\n",
    "from util.BuildData import TrainingSet_NN_Prices, TrainingSet_NN_Logret\n",
    "from util.BuildData import TrainingSet_ML_Prices, TrainingSet_ML_Logret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    def __init__(self, dx = 8.448750494999999e-06, dy = 8.44596043e-06, bound = 0.002):\n",
    "        \n",
    "        self.dx = dx\n",
    "        self.dy = dy\n",
    "        \n",
    "        self.xmin = -math.fabs(bound)\n",
    "        self.xmax =  math.fabs(bound)\n",
    "        \n",
    "        self.ymin = -math.fabs(bound)\n",
    "        self.ymax =  math.fabs(bound)\n",
    "        \n",
    "        self.__recalculate()        \n",
    "        pass\n",
    "    \n",
    "    def fit(self, X):        \n",
    "        labels   = []\n",
    "        clusters = []\n",
    "        \n",
    "        for i in range(len(X)):            \n",
    "            _, _, index, cell = self.cell(X[i])\n",
    "            \n",
    "            labels.append(index)\n",
    "            clusters.append(cell)    \n",
    "        \n",
    "        labels   = np.reshape(labels,   (len(labels),  1))\n",
    "        clusters = np.reshape(clusters, (len(clusters),2))\n",
    "        \n",
    "        return labels, clusters\n",
    "    \n",
    "    def fit_cells(self, X):\n",
    "        cells = []\n",
    "        \n",
    "        for i in range(len(X)):            \n",
    "            i, j, _, _ = self.cell(X[i])\n",
    "            cells.append([i, j])\n",
    "        \n",
    "        cells = np.reshape(cells, (len(cells),2))        \n",
    "        return cells\n",
    "    \n",
    "    def cell(self, v):\n",
    "        dims = math.sqrt(grid.shape()[0])\n",
    "        \n",
    "        dy   = self.dy\n",
    "        dy_2 = dy/2\n",
    "        \n",
    "        dx   = self.dx\n",
    "        dx_2 = dx/2\n",
    "        \n",
    "        v0 = v[0]\n",
    "        v1 = v[1]\n",
    "        \n",
    "        sv0 = 0\n",
    "        if v0 != 0.0:\n",
    "            sv0 = int(v0 / math.fabs(v0))\n",
    "        \n",
    "        sv1 = 0\n",
    "        if v1 != 0.0:\n",
    "            sv1 = int(v1 / math.fabs(v1))\n",
    "        \n",
    "        i0 = int((v0 + sv0 * dy_2) / dy) \n",
    "        j0 = int((v1 + sv1 * dx_2) / dx) \n",
    "        \n",
    "        i = i0 + int(dims/2)\n",
    "        j = j0 + int(dims/2)\n",
    "\n",
    "        index = int(i*dims + j)\n",
    "        cell  = self.grid[index]\n",
    "        \n",
    "        return i0, j0, index, cell\n",
    "    \n",
    "    def __recalculate(self):\n",
    "        x = 0\n",
    "        grid_x = []\n",
    "        grid_x.append(x)\n",
    "\n",
    "        while x < self.xmax:        \n",
    "            x = x + self.dx\n",
    "            grid_x.append(x)\n",
    "            grid_x.append(-x)\n",
    "    \n",
    "        y = 0\n",
    "        grid_y = []\n",
    "        grid_y.append(y)\n",
    "\n",
    "        while y < self.ymax:\n",
    "            y = y + self.dy\n",
    "            grid_y.append(y)\n",
    "            grid_y.append(-y)\n",
    "    \n",
    "        grid_x.sort()\n",
    "        grid_y.sort()\n",
    "\n",
    "        grid = []\n",
    "\n",
    "        for i in range(0,len(grid_y)):\n",
    "            for j in range(0,len(grid_x)):\n",
    "                grid.append([grid_y[i], grid_x[j]])\n",
    "\n",
    "        self.grid = np.array(grid)\n",
    "        pass\n",
    "        \n",
    "    def plot(self, show = False):\n",
    "        G = self.grid.T        \n",
    "        plt.scatter(G[0], G[1], color='red')\n",
    "        \n",
    "        if show:\n",
    "            plt.show()\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def shape(self):\n",
    "        return self.grid.shape\n",
    "\n",
    "def metric(x):\n",
    "    return max(math.fabs(x[0]), math.fabs(x[1]))\n",
    "\n",
    "def build_set(S, lag = 2):\n",
    "    return hankel(S[0 : lag], S[lag-1 :]).T\n",
    "\n",
    "def plot_phase_space(S, Lag, n):\n",
    "    m = n/2;\n",
    "    c = 16  \n",
    "    \n",
    "    ################################\n",
    "    \n",
    "    X = build_set(S)\n",
    "    \n",
    "    #X0 = build_set(S)\n",
    "    #X = []\n",
    "    \n",
    "    #for i in range(len(X0)):\n",
    "    #    if metric(X0[i]) < 0.00001:\n",
    "    #        X.append(X0[i])\n",
    "\n",
    "    #X = np.array(X)\n",
    "    \n",
    "    \n",
    "    clf = MeanShift(bandwidth=0.000004, bin_seeding=True)\n",
    "    clf.fit_predict(X)\n",
    "\n",
    "    cluster_centers = clf.cluster_centers_.T\n",
    "    \n",
    "    X = X.T\n",
    "    \n",
    "    x0 = X[0][0:m-c]\n",
    "    y0 = X[1][0:m-c]\n",
    "    \n",
    "    x1 = X[0][m-c:m]\n",
    "    y1 = X[1][m-c:m]\n",
    "    \n",
    "    x2 = X[0][m:n]\n",
    "    y2 = X[1][m:n]  \n",
    "    \n",
    "    plt.scatter(x0,y0)\n",
    "    plt.plot(x1,y1, 'b*--')\n",
    "    plt.scatter(x2,y2)\n",
    "    plt.plot(cluster_centers[0], cluster_centers[1], 'o', markersize=6, markerfacecolor='red')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print clf.cluster_centers_\n",
    "    print clf.labels_\n",
    "    \n",
    "def plot_phase_space0(S, n, Lag = 2):\n",
    "    m = n/2;\n",
    "    c = 16\n",
    "    \n",
    "    X = build_set(S, Lag)\n",
    "    X = X.T\n",
    "    \n",
    "    x0 = X[0][0:m-c]\n",
    "    y0 = X[1][0:m-c]\n",
    "    \n",
    "    x1 = X[0][m-c:m]\n",
    "    y1 = X[1][m-c:m]\n",
    "    \n",
    "    x2 = X[0][m:n]\n",
    "    y2 = X[1][m:n]  \n",
    "    \n",
    "    plt.scatter(x0,y0)\n",
    "    plt.plot(x1,y1, 'b*--')\n",
    "    plt.scatter(x2,y2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recurrent_neural_network_model(input):\n",
    "\n",
    "    layer = {'w' : tf.Variable(tf.random_normal([num_hidden, n_classes])),\n",
    "             'b' : tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    input = tf.unstack(input, timesteps, 1)    \n",
    "    \n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden)    \n",
    "    outs, state = rnn.static_rnn(lstm_cell, input, dtype=tf.float32)\n",
    "    \n",
    "    output = tf.add(tf.matmul(outs[-1], layer['w']), layer['b'], name='nn')    \n",
    "    return output\n",
    "\n",
    "def train_and_save_nn(x_data,y_data, filename):\n",
    "    x_train = tf.placeholder('float', [None, timesteps, num_input], name='x')\n",
    "    y_train = tf.placeholder('float', [None, n_classes], name='y')\n",
    "    \n",
    "    nn = recurrent_neural_network_model(x_train)            \n",
    "    cost = tf.norm(tf.subtract(nn, y_train), name='cost')\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001).minimize(cost)   \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(cost)   \n",
    "    \n",
    "    #correct = tf.equal(tf.floor(nn), y_train, name='correct')\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct, 'float'), name = 'accuracy')\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        \n",
    "        session.run(tf.initialize_all_variables())\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for i in range(int(len(X_train)/batch_size)):\n",
    "                \n",
    "                batch_x = x_data[i*batch_size : (i+1)*batch_size]\n",
    "                batch_y = y_data[i*batch_size : (i+1)*batch_size]\n",
    "               \n",
    "                batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                \n",
    "                _, c = session.run([optimizer, cost], feed_dict = {x_train: batch_x, y_train: batch_y})\n",
    "                epoch_loss += c\n",
    "                \n",
    "            if epoch % 1000 == 0:\n",
    "                print('Epoch', epoch, 'completed out of', hm_epochs, 'loss', epoch_loss)\n",
    "               \n",
    "        #saver.save(session, 'data/fxrn.ckpt')\n",
    "        saver.save(session, filename)\n",
    "        print('Last epoch loss: ', epoch_loss)\n",
    "        \n",
    "    return nn#, correct, accuracy\n",
    "\n",
    "def test_nn(x_test, y_test, filename):\n",
    "    with tf.Session() as session:\n",
    "        #saver = tf.train.import_meta_graph('data/fxrn.ckpt.meta')\n",
    "        saver = tf.train.import_meta_graph(filename)\n",
    "        saver.restore(session, tf.train.latest_checkpoint('data/'))\n",
    "        \n",
    "        graph    = tf.get_default_graph()        \n",
    "                \n",
    "        x  = graph.get_tensor_by_name('x:0')\n",
    "        y  = graph.get_tensor_by_name('y:0')\n",
    "        \n",
    "        nn = graph.get_tensor_by_name('nn:0')        \n",
    "\n",
    "        inputs  = x_test        \n",
    "        inputs  = np.reshape(inputs, (testsize, timesteps, num_input))\n",
    "        \n",
    "        outputs = session.run([nn], feed_dict = {x: inputs})\n",
    "        predicted = outputs[0]        \n",
    "        \n",
    "        #x0 = y_test[0]\n",
    "        #x0 = np.reshape(x0, (1, n_classes))\n",
    "        \n",
    "        #y0 = predicted[0]\n",
    "        #y0 = np.reshape(y0, (1, n_classes))\n",
    "        \n",
    "        #x_ = tf.placeholder('float', [1, n_classes])\n",
    "        #y_ = tf.placeholder('float', [1, n_classes])        \n",
    "        \n",
    "        #print x0\n",
    "        #print y0\n",
    "        \n",
    "        #cost = tf.norm(tf.subtract(x_, y_))\n",
    "        #output = session.run([cost], feed_dict = {x_: x0, y_: y0})\n",
    "        #print output\n",
    "        \n",
    "        # Visualising the results\n",
    "        \n",
    "        print y_test\n",
    "        print predicted\n",
    "        \n",
    "        plt.plot(y_test, 'b*',    label = 'Actual')\n",
    "        plt.plot(predicted, 'r+', label = 'Predicted')\n",
    "\n",
    "        plt.title('Prediction')        \n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "\n",
    "N = 8*1440 + 1\n",
    "\n",
    "source = pd.read_csv('EURUSD1.csv', header=0) # source CSV\n",
    "prices = np.array(source.Close)[0:N+1] # close prices\n",
    "r = CalcReturns(prices)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = build_set(r)\n",
    "\n",
    "grid = Grid(bound = 0.004)\n",
    "#grid.plot()\n",
    "\n",
    "#plot_phase_space0(r, N-1)\n",
    "#plt.show()\n",
    "\n",
    "cells = grid.fit_cells(X)\n",
    "\n",
    "#plt.plot(labels)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timesteps  = 1\n",
    "num_input  = 2\n",
    "n_classes  = 1\n",
    "step       = num_input * timesteps\n",
    "\n",
    "lag        = 1\n",
    "testsize   = 16\n",
    "trainsize  = 1024 #N - testsize\n",
    "\n",
    "hm_epochs  = 100000\n",
    "num_hidden = 16\n",
    "batch_size = 516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "\n",
    "# Importing the training set\n",
    "data_set = cells[:trainsize + lag + testsize + lag + 1]\n",
    "#data_set = sc.fit_transform(data_set)\n",
    "\n",
    "testing_set  = data_set[-testsize-lag:]\n",
    "training_set = data_set[:trainsize+lag]\n",
    "\n",
    "# Getting the inputs and the ouputs\n",
    "X_train = training_set[0:trainsize]\n",
    "\n",
    "Y_train0 = training_set[lag:trainsize+lag].T[0]\n",
    "Y_train0 = np.reshape(Y_train0, (len(Y_train0), n_classes))\n",
    "\n",
    "Y_train1 = training_set[lag:trainsize+lag].T[1]\n",
    "Y_train1 = np.reshape(Y_train1, (len(Y_train1), n_classes))\n",
    "\n",
    "X_test = testing_set[0:testsize]\n",
    "\n",
    "Y_test0 = testing_set[lag:testsize+lag].T[0]\n",
    "Y_test0 = np.reshape(Y_test0, (len(Y_test0), n_classes))\n",
    "\n",
    "Y_test1 = testing_set[lag:testsize+lag].T[1]\n",
    "Y_test1 = np.reshape(Y_test1, (len(Y_test1), n_classes))\n",
    "\n",
    "# Reshaping\n",
    "X_train = np.reshape(X_train, (trainsize, timesteps, num_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-312e7fea891b>:30: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "('Epoch', 0, 'completed out of', 100000, 'loss', 166.05233764648438)\n",
      "('Epoch', 1000, 'completed out of', 100000, 'loss', 165.578369140625)\n",
      "('Epoch', 2000, 'completed out of', 100000, 'loss', 165.05308532714844)\n",
      "('Epoch', 3000, 'completed out of', 100000, 'loss', 164.70631408691406)\n",
      "('Epoch', 4000, 'completed out of', 100000, 'loss', 164.43280029296875)\n",
      "('Epoch', 5000, 'completed out of', 100000, 'loss', 164.19497680664062)\n",
      "('Epoch', 6000, 'completed out of', 100000, 'loss', 163.97859191894531)\n",
      "('Epoch', 7000, 'completed out of', 100000, 'loss', 163.77218627929688)\n",
      "('Epoch', 8000, 'completed out of', 100000, 'loss', 163.57551574707031)\n",
      "('Epoch', 9000, 'completed out of', 100000, 'loss', 163.38685607910156)\n",
      "('Epoch', 10000, 'completed out of', 100000, 'loss', 163.21221923828125)\n",
      "('Epoch', 11000, 'completed out of', 100000, 'loss', 163.05241394042969)\n",
      "('Epoch', 12000, 'completed out of', 100000, 'loss', 162.90693664550781)\n",
      "('Epoch', 13000, 'completed out of', 100000, 'loss', 162.76785278320312)\n",
      "('Epoch', 14000, 'completed out of', 100000, 'loss', 162.63996887207031)\n",
      "('Epoch', 15000, 'completed out of', 100000, 'loss', 162.52197265625)\n",
      "('Epoch', 16000, 'completed out of', 100000, 'loss', 162.40937805175781)\n",
      "('Epoch', 17000, 'completed out of', 100000, 'loss', 162.23553466796875)\n",
      "('Epoch', 18000, 'completed out of', 100000, 'loss', 162.097900390625)\n",
      "('Epoch', 19000, 'completed out of', 100000, 'loss', 161.92689514160156)\n",
      "('Epoch', 20000, 'completed out of', 100000, 'loss', 161.76362609863281)\n",
      "('Epoch', 21000, 'completed out of', 100000, 'loss', 161.62886047363281)\n",
      "('Epoch', 22000, 'completed out of', 100000, 'loss', 161.46949768066406)\n",
      "('Epoch', 23000, 'completed out of', 100000, 'loss', 161.35098266601562)\n",
      "('Epoch', 24000, 'completed out of', 100000, 'loss', 161.22845458984375)\n",
      "('Epoch', 25000, 'completed out of', 100000, 'loss', 161.08319091796875)\n",
      "('Epoch', 26000, 'completed out of', 100000, 'loss', 161.00483703613281)\n",
      "('Epoch', 27000, 'completed out of', 100000, 'loss', 160.93766784667969)\n",
      "('Epoch', 28000, 'completed out of', 100000, 'loss', 160.87600708007812)\n",
      "('Epoch', 29000, 'completed out of', 100000, 'loss', 160.81242370605469)\n",
      "('Epoch', 30000, 'completed out of', 100000, 'loss', 160.75505065917969)\n",
      "('Epoch', 31000, 'completed out of', 100000, 'loss', 160.70404052734375)\n",
      "('Epoch', 32000, 'completed out of', 100000, 'loss', 160.65773010253906)\n",
      "('Epoch', 33000, 'completed out of', 100000, 'loss', 160.61500549316406)\n",
      "('Epoch', 34000, 'completed out of', 100000, 'loss', 160.57341003417969)\n",
      "('Epoch', 35000, 'completed out of', 100000, 'loss', 160.53407287597656)\n",
      "('Epoch', 36000, 'completed out of', 100000, 'loss', 160.49729919433594)\n",
      "('Epoch', 37000, 'completed out of', 100000, 'loss', 160.46253967285156)\n",
      "('Epoch', 38000, 'completed out of', 100000, 'loss', 160.4293212890625)\n",
      "('Epoch', 39000, 'completed out of', 100000, 'loss', 160.3974609375)\n",
      "('Epoch', 40000, 'completed out of', 100000, 'loss', 160.36666870117188)\n",
      "('Epoch', 41000, 'completed out of', 100000, 'loss', 160.33648681640625)\n",
      "('Epoch', 42000, 'completed out of', 100000, 'loss', 160.30677795410156)\n",
      "('Epoch', 43000, 'completed out of', 100000, 'loss', 160.27743530273438)\n",
      "('Epoch', 44000, 'completed out of', 100000, 'loss', 160.24851989746094)\n",
      "('Epoch', 45000, 'completed out of', 100000, 'loss', 160.22003173828125)\n",
      "('Epoch', 46000, 'completed out of', 100000, 'loss', 160.19200134277344)\n",
      "('Epoch', 47000, 'completed out of', 100000, 'loss', 160.16436767578125)\n",
      "('Epoch', 48000, 'completed out of', 100000, 'loss', 160.13713073730469)\n",
      "('Epoch', 49000, 'completed out of', 100000, 'loss', 160.11032104492188)\n",
      "('Epoch', 50000, 'completed out of', 100000, 'loss', 160.0838623046875)\n",
      "('Epoch', 51000, 'completed out of', 100000, 'loss', 160.05783081054688)\n",
      "('Epoch', 52000, 'completed out of', 100000, 'loss', 160.03218078613281)\n",
      "('Epoch', 53000, 'completed out of', 100000, 'loss', 160.00688171386719)\n",
      "('Epoch', 54000, 'completed out of', 100000, 'loss', 159.98196411132812)\n",
      "('Epoch', 55000, 'completed out of', 100000, 'loss', 159.95738220214844)\n",
      "('Epoch', 56000, 'completed out of', 100000, 'loss', 159.93310546875)\n",
      "('Epoch', 57000, 'completed out of', 100000, 'loss', 159.90914916992188)\n",
      "('Epoch', 58000, 'completed out of', 100000, 'loss', 159.88554382324219)\n",
      "('Epoch', 59000, 'completed out of', 100000, 'loss', 159.86225891113281)\n",
      "('Epoch', 60000, 'completed out of', 100000, 'loss', 159.83929443359375)\n",
      "('Epoch', 61000, 'completed out of', 100000, 'loss', 159.81663513183594)\n",
      "('Epoch', 62000, 'completed out of', 100000, 'loss', 159.79429626464844)\n",
      "('Epoch', 63000, 'completed out of', 100000, 'loss', 159.77224731445312)\n",
      "('Epoch', 64000, 'completed out of', 100000, 'loss', 159.75048828125)\n",
      "('Epoch', 65000, 'completed out of', 100000, 'loss', 159.72903442382812)\n",
      "('Epoch', 66000, 'completed out of', 100000, 'loss', 159.70780944824219)\n",
      "('Epoch', 67000, 'completed out of', 100000, 'loss', 159.68692016601562)\n",
      "('Epoch', 68000, 'completed out of', 100000, 'loss', 159.66629028320312)\n",
      "('Epoch', 69000, 'completed out of', 100000, 'loss', 159.64590454101562)\n",
      "('Epoch', 70000, 'completed out of', 100000, 'loss', 159.62579345703125)\n",
      "('Epoch', 71000, 'completed out of', 100000, 'loss', 159.60592651367188)\n",
      "('Epoch', 72000, 'completed out of', 100000, 'loss', 159.58627319335938)\n",
      "('Epoch', 73000, 'completed out of', 100000, 'loss', 159.56689453125)\n",
      "('Epoch', 74000, 'completed out of', 100000, 'loss', 159.54776000976562)\n",
      "('Epoch', 75000, 'completed out of', 100000, 'loss', 159.52880859375)\n",
      "('Epoch', 76000, 'completed out of', 100000, 'loss', 159.51011657714844)\n",
      "('Epoch', 77000, 'completed out of', 100000, 'loss', 159.49162292480469)\n",
      "('Epoch', 78000, 'completed out of', 100000, 'loss', 159.47335815429688)\n",
      "('Epoch', 79000, 'completed out of', 100000, 'loss', 159.45526123046875)\n",
      "('Epoch', 80000, 'completed out of', 100000, 'loss', 159.43736267089844)\n",
      "('Epoch', 81000, 'completed out of', 100000, 'loss', 159.41963195800781)\n",
      "('Epoch', 82000, 'completed out of', 100000, 'loss', 159.40211486816406)\n",
      "('Epoch', 83000, 'completed out of', 100000, 'loss', 159.384765625)\n",
      "('Epoch', 84000, 'completed out of', 100000, 'loss', 159.36761474609375)\n",
      "('Epoch', 85000, 'completed out of', 100000, 'loss', 159.3505859375)\n",
      "('Epoch', 86000, 'completed out of', 100000, 'loss', 159.333740234375)\n",
      "('Epoch', 87000, 'completed out of', 100000, 'loss', 159.31704711914062)\n",
      "('Epoch', 88000, 'completed out of', 100000, 'loss', 159.30047607421875)\n",
      "('Epoch', 89000, 'completed out of', 100000, 'loss', 159.01141357421875)\n",
      "('Epoch', 90000, 'completed out of', 100000, 'loss', 158.67167663574219)\n",
      "('Epoch', 91000, 'completed out of', 100000, 'loss', 158.56680297851562)\n",
      "('Epoch', 92000, 'completed out of', 100000, 'loss', 158.50341796875)\n",
      "('Epoch', 93000, 'completed out of', 100000, 'loss', 158.45780944824219)\n",
      "('Epoch', 94000, 'completed out of', 100000, 'loss', 158.41961669921875)\n",
      "('Epoch', 95000, 'completed out of', 100000, 'loss', 158.38322448730469)\n",
      "('Epoch', 96000, 'completed out of', 100000, 'loss', 158.34068298339844)\n",
      "('Epoch', 97000, 'completed out of', 100000, 'loss', 158.27484130859375)\n",
      "('Epoch', 98000, 'completed out of', 100000, 'loss', 158.2039794921875)\n",
      "('Epoch', 99000, 'completed out of', 100000, 'loss', 158.14337158203125)\n",
      "('Last epoch loss: ', 158.09117126464844)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'nn:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_and_save_nn(X_train, Y_train0, 'data/fxrn_0.ckpt')\n",
    "train_and_save_nn(X_train, Y_train1, 'data/fxrn_1.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9]\n",
      " [ -1]\n",
      " [-18]\n",
      " [ 13]\n",
      " [ -4]\n",
      " [ 42]\n",
      " [-18]\n",
      " [-34]\n",
      " [-13]\n",
      " [-21]\n",
      " [  8]\n",
      " [ 12]\n",
      " [  0]\n",
      " [  2]\n",
      " [ -6]\n",
      " [ -4]]\n",
      "[[  8.80082321e+00]\n",
      " [ -9.38486934e-01]\n",
      " [ -1.65709152e+01]\n",
      " [  1.32752018e+01]\n",
      " [ -4.09826851e+00]\n",
      " [  1.93600292e+01]\n",
      " [ -1.65815964e+01]\n",
      " [ -1.72626724e+01]\n",
      " [ -1.36609297e+01]\n",
      " [ -1.70096531e+01]\n",
      " [  7.97237539e+00]\n",
      " [  1.19024010e+01]\n",
      " [ -9.79566574e-03]\n",
      " [  2.00588679e+00]\n",
      " [ -5.92997360e+00]\n",
      " [ -4.11725187e+00]]\n"
     ]
    }
   ],
   "source": [
    "#test_nn(X_test, Y_test0, 'data/fxrn_0.ckpt.meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1]\n",
      " [-18]\n",
      " [ 13]\n",
      " [ -4]\n",
      " [ 42]\n",
      " [-18]\n",
      " [-34]\n",
      " [-13]\n",
      " [-21]\n",
      " [  8]\n",
      " [ 12]\n",
      " [  0]\n",
      " [  2]\n",
      " [ -6]\n",
      " [ -4]\n",
      " [  2]]\n",
      "[[-0.77854323]\n",
      " [ 0.86279988]\n",
      " [ 4.60826921]\n",
      " [-0.23840949]\n",
      " [-1.85392976]\n",
      " [-0.23840949]\n",
      " [-1.85309553]\n",
      " [-0.23838411]\n",
      " [-0.23840949]\n",
      " [-0.23828019]\n",
      " [-0.23840949]\n",
      " [-0.8264401 ]\n",
      " [-4.25870466]\n",
      " [ 4.42886209]\n",
      " [-2.71083331]\n",
      " [-0.23676589]]\n"
     ]
    }
   ],
   "source": [
    "test_nn(X_test, Y_test1, 'data/fxrn_1.ckpt.meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
